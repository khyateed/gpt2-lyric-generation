{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with GPT-2 [Modeling]\n",
    "### Data Mining Final Project\n",
    "(Working through tutorials) - https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272<br>\n",
    "\n",
    "Khyatee Desai<br>\n",
    "kdesai1@gradcenter.cuny.edu<br>\n",
    "December 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Careless Whisper</td>\n",
       "      <td>/ivete-sangalo/careless-whisper.html</td>\n",
       "      <td>I feel so unsure\\nAs I take your hand and lead...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Could You Be Loved / Citação Musical do Rap: S...</td>\n",
       "      <td>/ivete-sangalo/could-you-be-loved-citacao-musi...</td>\n",
       "      <td>Don't let them fool, ya\\nOr even try to school...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Cruisin' (Part. Saulo)</td>\n",
       "      <td>/ivete-sangalo/cruisin-part-saulo.html</td>\n",
       "      <td>Baby, let's cruise, away from here\\nDon't be c...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Easy</td>\n",
       "      <td>/ivete-sangalo/easy.html</td>\n",
       "      <td>Know it sounds funny\\nBut, I just can't stand ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>For Your Babies (The Voice cover)</td>\n",
       "      <td>/ivete-sangalo/for-your-babies-the-voice-cover...</td>\n",
       "      <td>You've got that look again\\nThe one I hoped I ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379926</th>\n",
       "      <td>/clegg-johnny/</td>\n",
       "      <td>The Waiting</td>\n",
       "      <td>/clegg-johnny/the-waiting.html</td>\n",
       "      <td>Chorus\\nHere we stand waiting on the plain\\nDa...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379927</th>\n",
       "      <td>/clegg-johnny/</td>\n",
       "      <td>Too Early For The Sky</td>\n",
       "      <td>/clegg-johnny/too-early-for-the-sky.html</td>\n",
       "      <td>I nearly disappeared into the mouth of a croco...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379928</th>\n",
       "      <td>/clegg-johnny/</td>\n",
       "      <td>Warsaw 1943 (I Never Betrayed The Revolution)</td>\n",
       "      <td>/clegg-johnny/warsaw-1943-i-never-betrayed-the...</td>\n",
       "      <td>Amambuka, amambuka azothengisa izwe lakithi, i...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379929</th>\n",
       "      <td>/clegg-johnny/</td>\n",
       "      <td>When The System Has Fallen</td>\n",
       "      <td>/clegg-johnny/when-the-system-has-fallen.html</td>\n",
       "      <td>Sweat in the heat for days on end\\nwaiting for...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379930</th>\n",
       "      <td>/clegg-johnny/</td>\n",
       "      <td>Woman Be My Country</td>\n",
       "      <td>/clegg-johnny/woman-be-my-country.html</td>\n",
       "      <td>Here we stand on the edge of the day\\nFaces me...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191814 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ALink                                              SName  \\\n",
       "69      /ivete-sangalo/                                   Careless Whisper   \n",
       "86      /ivete-sangalo/  Could You Be Loved / Citação Musical do Rap: S...   \n",
       "88      /ivete-sangalo/                             Cruisin' (Part. Saulo)   \n",
       "111     /ivete-sangalo/                                               Easy   \n",
       "140     /ivete-sangalo/                  For Your Babies (The Voice cover)   \n",
       "...                 ...                                                ...   \n",
       "379926   /clegg-johnny/                                        The Waiting   \n",
       "379927   /clegg-johnny/                              Too Early For The Sky   \n",
       "379928   /clegg-johnny/      Warsaw 1943 (I Never Betrayed The Revolution)   \n",
       "379929   /clegg-johnny/                         When The System Has Fallen   \n",
       "379930   /clegg-johnny/                                Woman Be My Country   \n",
       "\n",
       "                                                    SLink  \\\n",
       "69                   /ivete-sangalo/careless-whisper.html   \n",
       "86      /ivete-sangalo/could-you-be-loved-citacao-musi...   \n",
       "88                 /ivete-sangalo/cruisin-part-saulo.html   \n",
       "111                              /ivete-sangalo/easy.html   \n",
       "140     /ivete-sangalo/for-your-babies-the-voice-cover...   \n",
       "...                                                   ...   \n",
       "379926                     /clegg-johnny/the-waiting.html   \n",
       "379927           /clegg-johnny/too-early-for-the-sky.html   \n",
       "379928  /clegg-johnny/warsaw-1943-i-never-betrayed-the...   \n",
       "379929      /clegg-johnny/when-the-system-has-fallen.html   \n",
       "379930             /clegg-johnny/woman-be-my-country.html   \n",
       "\n",
       "                                                    Lyric language  \n",
       "69      I feel so unsure\\nAs I take your hand and lead...       en  \n",
       "86      Don't let them fool, ya\\nOr even try to school...       en  \n",
       "88      Baby, let's cruise, away from here\\nDon't be c...       en  \n",
       "111     Know it sounds funny\\nBut, I just can't stand ...       en  \n",
       "140     You've got that look again\\nThe one I hoped I ...       en  \n",
       "...                                                   ...      ...  \n",
       "379926  Chorus\\nHere we stand waiting on the plain\\nDa...       en  \n",
       "379927  I nearly disappeared into the mouth of a croco...       en  \n",
       "379928  Amambuka, amambuka azothengisa izwe lakithi, i...       en  \n",
       "379929  Sweat in the heat for days on end\\nwaiting for...       en  \n",
       "379930  Here we stand on the edge of the day\\nFaces me...       en  \n",
       "\n",
       "[191814 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prepare data\n",
    "lyrics = pd.read_csv('lyrics-data.csv')\n",
    "lyrics = lyrics[lyrics['language']=='en']\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's Up</td>\n",
       "      <td>Twenty-five years and my life is still\\nTrying...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spaceman</td>\n",
       "      <td>Starry night bring me down\\nTill I realize the...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pleasantly Blue</td>\n",
       "      <td>Every time you wake in the mornin'\\nAnd you st...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm The One</td>\n",
       "      <td>Ah-hah!\\nWoo!\\nAh-ha-ha-ha-ha-ha!\\nWe came her...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Mr. President</td>\n",
       "      <td>I'm looking outside of my windows\\nThe view th...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>Three Button Hand Me Down</td>\n",
       "      <td>(Ian McLagan, Rod Stewart)\\n\\nI don't need no ...</td>\n",
       "      <td>Faces</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>Too Bad</td>\n",
       "      <td>(Ron Wood, Rod Stewart)\\n\\nToo bad we were thr...</td>\n",
       "      <td>Faces</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>Wicked Messenger</td>\n",
       "      <td>(Bob Dylan)\\n\\nThere was a wicked messenger\\nf...</td>\n",
       "      <td>Faces</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>You Can Make Me Dance, Sing, Or Anything</td>\n",
       "      <td>(Jones, McLagan, Stewart, Wood, Yamauchi)\\n\\nh...</td>\n",
       "      <td>Faces</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>You're So Rude</td>\n",
       "      <td>(Ian McLagan, Ronnie Lane)\\n\\nMy mum she likes...</td>\n",
       "      <td>Faces</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2523 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         SName  \\\n",
       "0                                    What's Up   \n",
       "1                                     Spaceman   \n",
       "2                              Pleasantly Blue   \n",
       "3                                  I'm The One   \n",
       "4                           Dear Mr. President   \n",
       "...                                        ...   \n",
       "2518                 Three Button Hand Me Down   \n",
       "2519                                   Too Bad   \n",
       "2520                          Wicked Messenger   \n",
       "2521  You Can Make Me Dance, Sing, Or Anything   \n",
       "2522                            You're So Rude   \n",
       "\n",
       "                                                  Lyric         Artist Genres  \n",
       "0     Twenty-five years and my life is still\\nTrying...  4 Non Blondes   Rock  \n",
       "1     Starry night bring me down\\nTill I realize the...  4 Non Blondes   Rock  \n",
       "2     Every time you wake in the mornin'\\nAnd you st...  4 Non Blondes   Rock  \n",
       "3     Ah-hah!\\nWoo!\\nAh-ha-ha-ha-ha-ha!\\nWe came her...  4 Non Blondes   Rock  \n",
       "4     I'm looking outside of my windows\\nThe view th...  4 Non Blondes   Rock  \n",
       "...                                                 ...            ...    ...  \n",
       "2518  (Ian McLagan, Rod Stewart)\\n\\nI don't need no ...          Faces   Rock  \n",
       "2519  (Ron Wood, Rod Stewart)\\n\\nToo bad we were thr...          Faces   Rock  \n",
       "2520  (Bob Dylan)\\n\\nThere was a wicked messenger\\nf...          Faces   Rock  \n",
       "2521  (Jones, McLagan, Stewart, Wood, Yamauchi)\\n\\nh...          Faces   Rock  \n",
       "2522  (Ian McLagan, Ronnie Lane)\\n\\nMy mum she likes...          Faces   Rock  \n",
       "\n",
       "[2523 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only keep popular artists, with genre Rock/Pop and popularity high enough\n",
    "artists = pd.read_csv('artists-data.csv')\n",
    "names = artists[(artists['Genres'].isin(['Rock']))]['Artist']\n",
    "artists = artists[(artists.Artist.isin(names)) & artists.Popularity>0]\n",
    "df = lyrics.merge(artists[['Artist', 'Genres', 'Link']], left_on='ALink', right_on='Link', how='inner')\n",
    "df = df.drop(columns=['ALink','SLink','language','Link'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "df = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350)]\n",
    "\n",
    "#Create a very small test set to compare generated text with the reality\n",
    "test_set = df.sample(n = 200)\n",
    "df = df.loc[~df.index.isin(test_set.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()\n",
    "\n",
    "#For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['Lyric'] = test_set['Lyric'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in df['Lyric']:\n",
    "          self.lyrics.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))               \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]\n",
    "    \n",
    "dataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type=\"gpt2\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=5, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [03:05, 12.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(1.9169, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [03:10, 11.79it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(0.8079, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [03:07, 11.98it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(1.0327, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [02:59, 12.49it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(1.1394, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [03:03, 12.25it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.72s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.40s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.29s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.10s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.07s/it]\n",
      "100%|██████████| 1/1 [00:18<00:00, 18.33s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.12s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.08s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.14s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.21s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.41s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.06s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.34s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.22s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.64s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.75s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.79s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.61s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.90s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.75s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.11s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.51s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.20s/it]\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.77s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.52s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.29s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.54s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.46s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.78s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.46s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.57s/it]\n",
      "100%|██████████| 1/1 [00:29<00:00, 29.46s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.62s/it]\n",
      "100%|██████████| 1/1 [00:19<00:00, 19.18s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.03s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.36s/it]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.49s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.27s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.62s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.87s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.11s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.09s/it]\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.37s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.04s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.99s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.62s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  6.00s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.38s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.23s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.42s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.39s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.10s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.41s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.01s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.98s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.54s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.16s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.03s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.76s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.94s/it]\n",
      "100%|██████████| 1/1 [00:19<00:00, 19.17s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.01s/it]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.52s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.65s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.04s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.93s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.92s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.71s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.96s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.93s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.26s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.52s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.76s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.39s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.08s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.55s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.45s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.86s/it]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.17s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.65s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.44s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.22s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.27s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.52s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 11.00s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.62s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.14s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.82s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.16s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.51s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.03s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.92s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.72s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.09s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.14s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.20s/it]\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.35s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.81s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.72s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.76s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.66s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.16s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.47s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.51s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.72s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.08s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.10s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.52s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.57s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.01s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.91s/it]\n",
      "100%|██████████| 1/1 [00:18<00:00, 18.23s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.96s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.36s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.36s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.15s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.87s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.76s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.51s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.33s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.88s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.83s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.64s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.03s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.99s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.00s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.95s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.36s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.91s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.27s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.26s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.38s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.12s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.33s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.20s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.81s/it]\n",
      "100%|██████████| 1/1 [00:19<00:00, 19.94s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.20s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.99s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.54s/it]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.75s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.51s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.88s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.60s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.59s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.15s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.38s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.73s/it]\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.12s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.15s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.42s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.51s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.84s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.12s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.25s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.20s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.54s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 12.00s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.58s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.25s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.27s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.54s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.19s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.80s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.66s/it]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.92s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.81s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.97s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.97s/it]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.78s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.27s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.):\n",
    "    \n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "    generated_lyrics = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)\n",
    "        generated_lyrics.append(x)\n",
    "    return generated_lyrics\n",
    "\n",
    "#Run the functions to generate the lyrics\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "    a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "    b = ' '.join(a)\n",
    "    c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "    my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations\n",
    "\n",
    "\n",
    "#Finish the sentences when there is a point, remove after that\n",
    "# final=[]\n",
    "\n",
    "# for i in range(len(test_set)):\n",
    "#     to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "#     final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "# test_set['Generated_lyrics'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genres</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1349</td>\n",
       "      <td>One With You</td>\n",
       "      <td>Angel I want to</td>\n",
       "      <td>Santana</td>\n",
       "      <td>Rock</td>\n",
       "      <td>be one with you Spread your wings and let me i...</td>\n",
       "      <td>hear what you think about this article. Submi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1978</td>\n",
       "      <td>Made a Mess of Me</td>\n",
       "      <td>Yeah I felt so free Life's a breeze I didn't s...</td>\n",
       "      <td>Stereophonics</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Yeah You turned me over You stitched me up And...</td>\n",
       "      <td>In California Was bittersweet And in the morn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>799</td>\n",
       "      <td>Say Sons</td>\n",
       "      <td>Well, I got a little girl down by the river No...</td>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Rock</td>\n",
       "      <td>on, down by the river oh, come on, girl, it's ...</td>\n",
       "      <td>Well, I got a little girl down by the river No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>774</td>\n",
       "      <td>Ramrod</td>\n",
       "      <td>Hey, little dolly with the blue jeans on I wan...</td>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Rock</td>\n",
       "      <td>little girl I'll put my foot to the floor Give...</td>\n",
       "      <td>down here in a few days I'll be there tonight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743</td>\n",
       "      <td>Fault Lines</td>\n",
       "      <td>See those fault lines Laid out like land mines...</td>\n",
       "      <td>Tom Petty</td>\n",
       "      <td>Rock</td>\n",
       "      <td>of my own I've got a few of my own fault lines...</td>\n",
       "      <td>of my own I've got a few of my own fault line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1840</td>\n",
       "      <td>The Golden Rose</td>\n",
       "      <td>Well, The Golden Rose sailed with a broken man...</td>\n",
       "      <td>Tom Petty</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Many a-night I would think of her All alone Ch...</td>\n",
       "      <td>I was gone\\n\\n\\nNow I've told you stories and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1150</td>\n",
       "      <td>I Believe To My Soul</td>\n",
       "      <td>One of these days and it won't be long Youre g...</td>\n",
       "      <td>The Animals</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Oh yes, I believe I believe to my soul You're ...</td>\n",
       "      <td>I believe to my soul you're tryin' to beat me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1114</td>\n",
       "      <td>Many Rivers To Cross</td>\n",
       "      <td>Eric Burdon &amp; Animals - Many Rivers To Cross M...</td>\n",
       "      <td>The Animals</td>\n",
       "      <td>Rock</td>\n",
       "      <td>say why Well I guess, I have to try, yeah yeah...</td>\n",
       "      <td>say why Well I guess, I have to try Many rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>134</td>\n",
       "      <td>Even A Fool Learns To Love</td>\n",
       "      <td>There was a time, the laughing time I took my ...</td>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>On a clown and an angel so much in love I'll s...</td>\n",
       "      <td>On a clown and an angel so much in love I'll ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>555</td>\n",
       "      <td>Dedication</td>\n",
       "      <td>Well way back in the Bible time A cat named No...</td>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Rock</td>\n",
       "      <td>through and through Dedication like I give to ...</td>\n",
       "      <td>to the pope I'm done now I'm done now I'm don...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                       SName  \\\n",
       "0     1349                One With You   \n",
       "1     1978           Made a Mess of Me   \n",
       "2      799                    Say Sons   \n",
       "3      774                      Ramrod   \n",
       "4     1743                 Fault Lines   \n",
       "..     ...                         ...   \n",
       "195   1840             The Golden Rose   \n",
       "196   1150        I Believe To My Soul   \n",
       "197   1114        Many Rivers To Cross   \n",
       "198    134  Even A Fool Learns To Love   \n",
       "199    555                  Dedication   \n",
       "\n",
       "                                                 Lyric             Artist  \\\n",
       "0                                      Angel I want to            Santana   \n",
       "1    Yeah I felt so free Life's a breeze I didn't s...      Stereophonics   \n",
       "2    Well, I got a little girl down by the river No...  Bruce Springsteen   \n",
       "3    Hey, little dolly with the blue jeans on I wan...  Bruce Springsteen   \n",
       "4    See those fault lines Laid out like land mines...          Tom Petty   \n",
       "..                                                 ...                ...   \n",
       "195  Well, The Golden Rose sailed with a broken man...          Tom Petty   \n",
       "196  One of these days and it won't be long Youre g...        The Animals   \n",
       "197  Eric Burdon & Animals - Many Rivers To Cross M...        The Animals   \n",
       "198  There was a time, the laughing time I took my ...        David Bowie   \n",
       "199  Well way back in the Bible time A cat named No...  Bruce Springsteen   \n",
       "\n",
       "    Genres                                    True_end_lyrics  \\\n",
       "0     Rock  be one with you Spread your wings and let me i...   \n",
       "1     Rock  Yeah You turned me over You stitched me up And...   \n",
       "2     Rock  on, down by the river oh, come on, girl, it's ...   \n",
       "3     Rock  little girl I'll put my foot to the floor Give...   \n",
       "4     Rock  of my own I've got a few of my own fault lines...   \n",
       "..     ...                                                ...   \n",
       "195   Rock  Many a-night I would think of her All alone Ch...   \n",
       "196   Rock  Oh yes, I believe I believe to my soul You're ...   \n",
       "197   Rock  say why Well I guess, I have to try, yeah yeah...   \n",
       "198   Rock  On a clown and an angel so much in love I'll s...   \n",
       "199   Rock  through and through Dedication like I give to ...   \n",
       "\n",
       "                                      Generated_lyrics  \n",
       "0     hear what you think about this article. Submi...  \n",
       "1     In California Was bittersweet And in the morn...  \n",
       "2    Well, I got a little girl down by the river No...  \n",
       "3     down here in a few days I'll be there tonight...  \n",
       "4     of my own I've got a few of my own fault line...  \n",
       "..                                                 ...  \n",
       "195   I was gone\\n\\n\\nNow I've told you stories and...  \n",
       "196   I believe to my soul you're tryin' to beat me...  \n",
       "197   say why Well I guess, I have to try Many rive...  \n",
       "198   On a clown and an angel so much in love I'll ...  \n",
       "199   to the pope I'm done now I'm done now I'm don...  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    reference = test_set['True_end_lyrics'][i]\n",
    "    candidate = test_set['Generated_lyrics'][i]\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
